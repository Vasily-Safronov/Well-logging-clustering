{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lasio\n",
    "import skfda\n",
    "from skfda.representation.basis import BSpline\n",
    "from skfda.representation.grid import FDataGrid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from clustergram import Clustergram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"train/\"\n",
    "train  = pd.read_csv(path + \"train_data.csv\").iloc[:,:-1]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {item:i for i, item in enumerate(train.WELL_NAME.unique())}\n",
    "train.WELL_NAME = train.WELL_NAME.apply(lambda x: \"well_\"+str(mapping[x]))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переименовываем названия las файлов которые есть в train\n",
    "\n",
    "for i in os.listdir('LAS'):\n",
    "    a = os.path.splitext(i)[0]\n",
    "    try:\n",
    "        os.rename('LAS/' + i, 'LAS/' + f'well{mapping[a]}.las')\n",
    "    except:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# находим виды каротажей, которые присутствуют во всех ГИС\n",
    "\n",
    "col_list = []\n",
    "\n",
    "for las_file in os.listdir('LAS'):\n",
    "    if 'well' in las_file:\n",
    "        las = lasio.read(\"LAS/\" + las_file)\n",
    "        df = las.df()\n",
    "        col_list.append(list(df.columns))\n",
    "\n",
    "col_list = list(set(col_list[0]).intersection(*col_list[1:]))\n",
    "col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_basis(y_vals, patience_corr=3, threshold_corr=0.01, patience_area_delta=3, threshold_area_delta=0.1):\n",
    "    \n",
    "    \"\"\"Функция для нахождения оптимального количества базисов для BSpline\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    y: array_like\n",
    "       1D массив\n",
    "    patience_corr: количество итераций без улучшений, после которых вычисление корреляции Пирсона будет остановлено. По умолчанию 3.\n",
    "    threshold_corr: порог, при котором процентная разница между значениями корреляции Пирсона  будет считаться незначительной. По умолчанию 0.01 (1%).\n",
    "    \n",
    "    patience_area_delta: количество итераций без улучшений, после которых вычисление дельты будет остановлено. По умолчанию 3.\n",
    "    threshold_area_delta: порог, при котором процентная разница между дельтами будет считаться незначительной. По умолчанию 0.1 (10%).\n",
    "    \n",
    "    Возвращает\n",
    "    -------\n",
    "    num_basis: оптимальное количество базисов\n",
    "    \"\"\"\n",
    "    \n",
    "    y = (y_vals - np.min(y_vals)) / (np.max(y_vals) - np.min(y_vals))\n",
    "    \n",
    "    grid_points = list(np.linspace(0, 1, len(y)))\n",
    "    \n",
    "    fd_orig = skfda.FDataGrid(data_matrix=y_vals, grid_points=grid_points)\n",
    "    \n",
    "    area_true = abs(integrate.simps(y=y, x=grid_points))\n",
    "    \n",
    "    func_data = []\n",
    "    \n",
    "    n_basis = 4\n",
    "    const = 1\n",
    "\n",
    "    corr_list = []\n",
    "    prev_corr= None\n",
    "    delta_corr_num = 0\n",
    "    \n",
    "    area_perc_list = []\n",
    "    delta_area_num = 0\n",
    "    \n",
    "    basis_list = []\n",
    "    \n",
    "    while True:\n",
    "        basis = BSpline(n_basis=n_basis)\n",
    "        fd_basis_orig = fd_orig.to_basis(basis)\n",
    "        \n",
    "        data_prd_orig = fd_basis_orig.to_grid(grid_points).data_matrix.reshape(-1)\n",
    "        current_corr = stats.pearsonr(data_prd_orig, y_vals)[0]\n",
    "        \n",
    "        data_prd_scal = (data_prd_orig - np.min(data_prd_orig)) / (np.max(data_prd_orig) - np.min(data_prd_orig))\n",
    "        area_pred = abs(integrate.simps(y=data_prd_scal, x=grid_points))\n",
    "        current_area_perc = 1 - (min(area_true, area_pred) / max(area_true, area_pred))\n",
    "        \n",
    "        if prev_corr is None:\n",
    "            corr_list.append(current_corr)\n",
    "            prev_corr = current_corr\n",
    "            \n",
    "            area_perc_list.append(current_area_perc)\n",
    "            \n",
    "            basis_list.append(n_basis)\n",
    "            n_basis += const\n",
    "            \n",
    "        else:\n",
    "            perc_corr_diff = 1 - (min(current_corr, prev_corr) / max(current_corr, prev_corr))\n",
    "            \n",
    "            if perc_corr_diff <= threshold_corr:\n",
    "                delta_corr_num += 1\n",
    "            else:\n",
    "                delta_corr_num = 0\n",
    "            \n",
    "            if current_area_perc <= threshold_area_delta:\n",
    "                delta_area_num += 1\n",
    "            else:\n",
    "                delta_area_num = 0\n",
    "            \n",
    "            corr_list.append(current_corr)\n",
    "            prev_corr = current_corr\n",
    "            \n",
    "            area_perc_list.append(current_area_perc)\n",
    "        \n",
    "            basis_list.append(n_basis)\n",
    "            n_basis += const\n",
    "        \n",
    "        if delta_area_num >= patience_area_delta and delta_corr_num >= patience_corr:\n",
    "            break\n",
    "    \n",
    "    # # for mean\n",
    "    ind = min(delta_corr_num, delta_area_num) + (abs(delta_corr_num - delta_area_num)//2)\n",
    "    \n",
    "    correlation_m = corr_list[-ind]\n",
    "    delta_m = area_perc_list[-ind]\n",
    "    n_basis_m = basis_list[-ind]\n",
    "    \n",
    "    basis = BSpline(n_basis=n_basis_m)\n",
    "    fd_basis = fd_orig.to_basis(basis)\n",
    "    \n",
    "    # for delta\n",
    "    delta_d = area_perc_list[-delta_area_num]\n",
    "    correlation_d = corr_list[-delta_area_num]\n",
    "    n_basis_d = basis_list[-delta_area_num]\n",
    "    \n",
    "    basis = BSpline(n_basis=n_basis_d)\n",
    "    fd_basis = fd_orig.to_basis(basis)\n",
    "    \n",
    "    # # for corr\n",
    "    delta_c = area_perc_list[-delta_corr_num]\n",
    "    correlation_c = corr_list[-delta_corr_num]\n",
    "    n_basis_c = basis_list[-delta_corr_num]\n",
    "    \n",
    "    basis = BSpline(n_basis=n_basis_c)\n",
    "    fd_basis = fd_orig.to_basis(basis)\n",
    "    \n",
    "    num_basis = max([n_basis_d, n_basis_c, n_basis_m])\n",
    "    \n",
    "    return num_basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск оптимального количества базисов для всех видов каротажей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Для каждой кривой находится оптимальное количество базисов\n",
    "2. Для всех кривых применяется аппроксимация BSpline с максимальным количеством базисов найденным среди кривых\n",
    "3. Получаем коэффициенты, характеризующие кривые"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_files = os.listdir('LAS')\n",
    "\n",
    "coeffs = []\n",
    "\n",
    "for col_name in col_list:\n",
    "    \n",
    "    las_data = []\n",
    "    \n",
    "    for las_file in lst_files:\n",
    "        \n",
    "        if 'well' in las_file:\n",
    "            las = lasio.read(\"LAS/\" + las_file)\n",
    "            df = las.df()\n",
    "            sc = MinMaxScaler(feature_range=(-1,1))\n",
    "            df = sc.fit_transform(df[col_name].values.reshape(-1, 1)).reshape(-1)\n",
    "            las_data.append(df)\n",
    "\n",
    "    mx_len = len(max(las_data, key=len))\n",
    "    grid_points_intpl = np.linspace(0, 1, mx_len)\n",
    "\n",
    "    n_basis_lst = []\n",
    "    intrp_curves = []\n",
    "    \n",
    "    for curve in las_data:\n",
    "        grid_points = np.linspace(0, 1, len(curve))\n",
    "        a = FDataGrid(data_matrix=curve, grid_points=grid_points).to_grid(grid_points_intpl)\n",
    "        n_bas  = find_num_basis(curve)\n",
    "        # n_bas  = find_num_basis(a.data_matrix.reshape(-1))\n",
    "        n_basis_lst.append(n_bas)\n",
    "        intrp_curves.append(a)\n",
    "    \n",
    "    n_bas = max(n_basis_lst)\n",
    "    \n",
    "    data = 0\n",
    "\n",
    "    for lst in intrp_curves:\n",
    "        if data == 0:\n",
    "            data = lst\n",
    "        else:\n",
    "            data = data.concatenate(lst)\n",
    "            \n",
    "    print(col_name)\n",
    "    print('number of basis', n_bas)\n",
    "    data.plot()\n",
    "    plt.show()\n",
    "    \n",
    "    basis = BSpline(n_basis=int(n_bas))\n",
    "    data_basis = data.to_basis(basis)\n",
    "    data_basis.plot()\n",
    "    plt.show()\n",
    "\n",
    "    coeffs.append(data_basis.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединяем коэффициенты по всем видам каротажей в один вектор признаков\n",
    "\n",
    "dat = []\n",
    "\n",
    "for lst_1, lst_2, lst_3 in zip(coeffs[0], coeffs[1], coeffs[2]):\n",
    "    dat.append(np.hstack((lst_1, lst_2, lst_3)))\n",
    "\n",
    "coeffs = dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "# снижаем размерность с потерей информации не выше 10%\n",
    "for num_comptnts in range(2, len(coeffs[0])):\n",
    "    \n",
    "    dd = pd.DataFrame(coeffs)\n",
    "\n",
    "    pca = PCA(n_components=num_comptnts)\n",
    "    pca_data = pca.fit_transform(dd)\n",
    "\n",
    "    total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "    \n",
    "    if total_var >= 90:\n",
    "        print(f'number of components: {num_comptnts}, with total_var={total_var}')\n",
    "        break\n",
    "\n",
    "pca_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgram = Clustergram(range(1, 7))\n",
    "cgram.fit(np.array(coeffs))\n",
    "cgram.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgram = Clustergram(range(1, 7), method='hierarchical', linkage='ward')\n",
    "cgram.fit(np.array(coeffs))\n",
    "cgram.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "silhouette = []\n",
    "\n",
    "K = range(1, 10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(coeffs)\n",
    "    distortions.append(kmeanModel.inertia_)\n",
    "    if k > 1:\n",
    "        silhouette.append(silhouette_score(coeffs, kmeanModel.labels_))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(K[1:], silhouette, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Silhouette')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем переменную coeffs для кластеризации\n",
    "\n",
    "result = KMeans(2).fit_predict(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация результатов кластеризации с помощью PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(coeffs)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca_data = pca.fit_transform(dd)\n",
    "\n",
    "dd = pd.DataFrame(pca_data[:, 0])\n",
    "\n",
    "dd[1] = pca_data[:, 1]\n",
    "\n",
    "dd[2] = result\n",
    "\n",
    "dd[2] = dd[2].astype(\"category\")\n",
    "\n",
    "fig = px.scatter(dd, x=0, y=1,color=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(coeffs)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_data = pca.fit_transform(dd)\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "dd = pd.DataFrame(pca_data[:, 0])\n",
    "\n",
    "dd[1] = pca_data[:, 1]\n",
    "\n",
    "dd[2] = pca_data[:, 2]\n",
    "\n",
    "dd[3] = result\n",
    "\n",
    "dd[3] = dd[3].astype(\"category\")\n",
    "\n",
    "fig = px.scatter_3d(dd, x=0, y=1, z=2, color=3,\n",
    "                    title=f'Total Explained Variance: {total_var:.2f}%'\n",
    "                    )\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('fda_clstrng')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe65226124dd345b266366e3302ecf7188a8c9cd4bab00efddda91e181d988d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
